# Evaluating the Capacity of Large Language Models to Interpret Emotions in Images

### Abstract:
The integration of artificial intelligence, particularly large language models (LLMs), in emotional stimulus selection and validation presents opportunities for enhancing emotion comprehension frameworks. Traditional methods for selecting and validating emotional stimuli are labor-intensive and prone to biases, thus necessitating
efficient and scalable alternatives. This study investigates the capability of LLMs, specifically GPT-4, in recognizing and rating emotions derived from visual stimuli.We focus on two primary dimensions of emotional response: valence (positive, neutral, or negative) and arousal (calm, neutral, or stimulated). By benchmarking the performance of
GPT-4 against human evaluations using the well-established Geneva Affective PicturE Database (GAPED), we aim to provide insights into the modelâ€™s potential as a reliable tool for automating the selection and validation of emotion elicitation stimuli.Our findings reveal thatGPT-4 approximates human ratings closely, particularly under few-shot
learning conditions. We observed some difficulties in accurately classifying subtler emotional cues, indicating room for improvement. These results highlight the potential to significantly streamline the process of emotional stimulus selection and validation, thus reducing the time and labor associated with traditional methodologies.


### Repo Folder:

** 1. Image ratings:

** 2.
